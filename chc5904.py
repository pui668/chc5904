import streamlit as st
import pandas as pd
import networkx as nx
from pyvis.network import Network
import matplotlib.pyplot as plt
import io

# --- Page Title ---
st.title("ðŸ“– Character Interaction Network Analysis")

# --- Introduction ---
st.markdown("""
## ðŸ“Š Introduction
In this project, we analyze the relationships between characters in a classic Chinese text using **network analysis** techniques. 
The goal is to uncover the **central characters**, their **communities**, and the **strength of their interactions**.

This report walks through:
1. **The Raw Data**: Displaying the raw character interactions.
2. **Network Graph**: Visualizing character relationships as a social network.
3. **Centrality Metrics**: Identifying the most influential characters.
4. **The Code**: Explaining how the analysis was done.
""")

# --- Embedded CSV Data ---
st.markdown("### Step 1: Raw Data")
st.markdown("This dataset contains character interactions, showing who interacts with whom and how frequently.")
csv_data = """
Source,Target,Weight
è³ˆç,è³ˆè“‰,7
è³ˆæ•¬,è³ˆç,1
è³ˆæ•¬,è³ˆè“‰,1
çŽ‹å¤«äºº,è³ˆå¯¶çŽ‰,3
è³ˆæ¯,è³ˆç,2
çŽ‹å¤«äºº,è³ˆæ¯,9
çŽ‹å¤«äºº,è³ˆè“‰,1
çŽ‹å¤«äºº,çŽ‹ç†™é³³,11
ç§¦å¯å¿,è³ˆå¯¶çŽ‰,1
çŽ‹å¤«äºº,è³ˆç,2
è³ˆå¯¶çŽ‰,è³ˆç,3
ç§¦å¯å¿,è³ˆæ¯,1
çŽ‹å¤«äºº,ç§¦å¯å¿,1
çŽ‹ç†™é³³,è³ˆæ¯,4
å¹³å…’,çŽ‹ç†™é³³,2
è³ˆæ”¿,è³ˆèµ¦,5
è³ˆç,è³ˆèµ¦,7
è³ˆæ”¿,è³ˆç,5
æž—é»›çŽ‰,è³ˆæ¯,1
æž—é»›çŽ‰,è³ˆç’‰,2
ç§¦å¯å¿,è³ˆç’‰,1
çŽ‹ç†™é³³,ç§¦å¯å¿,1
è¥²äºº,è³ˆæ¯,3
çŽ‹ç†™é³³,è³ˆç,3
çŽ‹ç†™é³³,è³ˆå¯¶çŽ‰,8
æž—é»›çŽ‰,çŽ‹ç†™é³³,1
çŽ‹ç†™é³³,è³ˆç’‰,3
çŽ‹å¤«äºº,è¿Žæ˜¥,2
å¯¶ç ,è³ˆç,1
è³ˆå¯¶çŽ‰,è³ˆæ¯,5
è–›å¯¶é‡µ,è¿Žæ˜¥,3
çŽ‹å¤«äºº,è–›å¯¶é‡µ,2
çŽ‹ç†™é³³,è–›å¯¶é‡µ,1
çŽ‹ç†™é³³,è¿Žæ˜¥,1
è³ˆæ¯,è³ˆè“‰,1
è³ˆè“‰,è³ˆèµ¦,1
è³ˆæ¯,è³ˆèµ¦,3
æž—é»›çŽ‰,è¿Žæ˜¥,2
æž—é»›çŽ‰,è³ˆå¯¶çŽ‰,9
è–›å¯¶é‡µ,è³ˆå¯¶çŽ‰,2
æž—é»›çŽ‰,è–›å¯¶é‡µ,4
è³ˆå¯¶çŽ‰,è¿Žæ˜¥,2
å¹³å…’,é¦™è±,2
çŽ‹ç†™é³³,é¦™è±,1
è³ˆç,è³ˆç’‰,6
å¹³å…’,è³ˆç’‰,1
çŽ‹ç†™é³³,è³ˆè“‰,2
è³ˆç’‰,è³ˆèµ¦,3
è³ˆæ”¿,è³ˆç’‰,2
è³ˆå¯¶çŽ‰,è³ˆæ”¿,3
è³ˆæ”¿,è³ˆæ¯,3
æž—é»›çŽ‰,çŽ‹å¤«äºº,1
è³ˆæŽ¢æ˜¥,è¿Žæ˜¥,2
è³ˆæ•¬,è³ˆèµ¦,1
è³¾æ”¿,è³¾æ•¬,1
è³ˆç,è³¾ç’°,1
è³¾ç’‰,è³¾ç’°,1
è³¾ç’°,è³¾è“‰,1
è³¾ç’‰,è³¾è“‰,1
è¥²äºº,è³¾å¯¶çŽ‰,4
ç§‹ç´‹,è³¾æ¯,1
å²å¤§å§‘å¨˜,è³¾æ¯,1
ç§‹ç´‹,é´›é´¦,1
è–›å¯¶é‡µ,é¦™è±,1
è³¾æƒœæ˜¥,è¿Žæ˜¥,1
æž—é»›çŽ‰,è³¾æŽ¢æ˜¥,1
è³¾æƒœæ˜¥,è³¾æŽ¢æ˜¥,1
è–›å¯¶é‡µ,è³¾æƒœæ˜¥,1
è–›å¯¶é‡µ,è³¾æŽ¢æ˜¥,1
æž—é»›çŽ‰,è³¾æƒœæ˜¥,1
è³¾æ¯,è³¾ç’°,1
è–›å¯¶é‡µ,è³¾ç’°,1
çŽ‹å¤«äºº,è³¾ç’°,1
çŽ‹ç†™é³³,è³¾ç’°,1
è±å…’,è¿Žæ˜¥,1
å²å¤§å§‘å¨˜,è–›å¯¶é‡µ,1
ç§¦å¯å¿,è³¾è“‰,2
"""

# --- Step 1: Load Data into a DataFrame ---
df = pd.read_csv(io.StringIO(csv_data))

# Display the raw data table
st.dataframe(df)

# --- Step 2: Build the Network ---
st.markdown("### Step 2: Network Visualization")
st.markdown("This interactive network graph shows the relationships between characters. The thickness of edges represents the frequency of interactions.")

# Create a NetworkX graph from the DataFrame
G = nx.Graph()
for index, row in df.iterrows():
    G.add_edge(row['Source'], row['Target'], weight=row['Weight'])

# Visualize the network using PyVis
net = Network(height="600px", width="100%", notebook=True)
net.from_nx(G)
net.show("network.html")

# Display the interactive PyVis network graph
st.write("Interactive Character Network:")
HtmlFile = open("network.html", 'r', encoding='utf-8')
source_code = HtmlFile.read()
st.components.v1.html(source_code, height=600)

# --- Step 3: Display Centrality Metrics ---
st.markdown("### Step 3: Centrality Metrics")
st.markdown("Here we calculate centrality metrics to identify key characters in the network.")

# Degree Centrality
degree_centrality = nx.degree_centrality(G)
st.subheader("Top 5 Characters by Degree Centrality")
top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
st.write(pd.DataFrame(top_degree, columns=["Character", "Degree Centrality"]))

# Betweenness Centrality
betweenness_centrality = nx.betweenness_centrality(G)
st.subheader("Top 5 Characters by Betweenness Centrality")
top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
st.write(pd.DataFrame(top_betweenness, columns=["Character", "Betweenness Centrality"]))

# --- Step 4: Code Walkthrough ---
st.markdown("### Step 4: Code Walkthrough")
st.markdown("In this section, we explain the code used to process the text and generate the network.")
st.markdown("""
1. **Text Processing**: We used **jieba** for tokenizing the text and identifying character names.
2. **Interaction Recording**: We recorded interactions between characters whenever they appeared in the same sentence.
3. **Network Construction**: We built a **NetworkX** graph to represent the relationships, using **PyVis** to create an interactive visualization.
4. **Centrality Calculations**: We used **NetworkX** to calculate metrics like **degree centrality** and **betweenness centrality** to identify key characters.
""")

code = '''
import jieba
import re
from collections import Counter
import csv

base_directory = r'C:\\Users\\puipu\\Documents\\MScGAH\\CHC5904\\Second Hands on Assignment'

with open(f'{base_directory}\\chapters_10_20.txt', 'r', encoding='utf-8') as file:
    text = file.read()

character_variations = {
    'è³ˆå¯¶çŽ‰': ['è³ˆå¯¶çŽ‰', 'å¯¶çŽ‰', 'çŽ‰å…„', 'è³ˆå…¬å­', 'è³ˆäºŒçˆº'],
    'æž—é»›çŽ‰': ['æž—é»›çŽ‰', 'é»›çŽ‰', 'æž—å¦¹å¦¹', 'æž—å§‘å¨˜', 'é»›'],
    'è–›å¯¶é‡µ': ['è–›å¯¶é‡µ', 'å¯¶é‡µ', 'è–›å°å§', 'è–›å§‘å¨˜', 'é‡µå§', 'è–›å§¨åª½'],
    'è¥²äºº': ['è¥²äºº', 'èŠ±è¥²äºº', 'è¥²å§‘å¨˜'],
    'é´›é´¦': ['é´›é´¦', 'é´›é´¦å§'],
    'è³ˆæ¯': ['è³ˆæ¯', 'è€ç¥–å®—', 'è³ˆè€å¤ªå›', 'è³ˆè€ç¥–', 'è€å¤ªå¤ª', 'å¤ªå›'],
    # Add more characters and their variations here...
}

def find_characters_in_window(window):
    present_characters = set()
    for main_character, variations in character_variations.items():
        if any(variation in window for variation in variations):
            present_characters.add(main_character)
    return present_characters

sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)

interactions = []

for sentence in sentences:
    tokens = jieba.lcut(sentence)  
    present_characters = find_characters_in_window(tokens) 
    
    if len(present_characters) > 1: 
        sorted_pairs = set()
        for character1 in present_characters:
            for character2 in present_characters:
                if character1 != character2: 
                    sorted_pair = tuple(sorted([character1, character2]))
                    sorted_pairs.add(sorted_pair)
        interactions.extend(sorted_pairs)

interaction_counts = Counter(interactions)

with open(f'{base_directory}\\character_interactions_with_nicknames.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(['Source', 'Target', 'Weight'])
    for (source, target), weight in interaction_counts.items():
        csvwriter.writerow([source, target, weight])

print("Character interactions saved to 'character_interactions_with_nicknames.csv'.")
'''
st.code(code, language="python")

st.markdown("This is the full Python code used for processing the text and generating the graph.")
