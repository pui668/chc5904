import streamlit as st
import pandas as pd
import networkx as nx
from pyvis.network import Network
import matplotlib.pyplot as plt
import io

# --- Page Title ---
st.title("ðŸ“– Character Interaction Network Analysis")

# --- Introduction ---
st.markdown("""
## ðŸ“Š Introduction
In this project, we analyze the relationships between characters in a classic Chinese text using **network analysis** techniques. 
The goal is to uncover the **central characters**, their **communities**, and the **strength of their interactions**.

This report walks through:
1. **The Raw Data**: Displaying the raw character interactions.
2. **Network Graph**: Visualizing character relationships as a social network.
3. **Centrality Metrics**: Identifying the most influential characters.
4. **The Code**: Explaining how the analysis was done.
5. **Comparison with Gephi**: Why the Gephi results are visually superior.
""")

# --- Embedded CSV Data ---
st.markdown("### Step 1: Raw Data")
st.markdown("This dataset contains character interactions, showing who interacts with whom and how frequently.")
csv_data = """
Source,Target,Weight
è³ˆç,è³ˆè“‰,7
è³¾æ•¬,è³¾ç,1
è³¾æ•¬,è³¾è“‰,1
çŽ‹å¤«äºº,è³¾å¯¶çŽ‰,3
è³¾æ¯,è³¾ç,2
çŽ‹å¤«äºº,è³¾æ¯,9
çŽ‹å¤«äºº,è³¾è“‰,1
çŽ‹å¤«äºº,çŽ‹ç†™é³³,11
ç§¦å¯å¿,è³¾å¯¶çŽ‰,1
çŽ‹å¤«äºº,è³¾ç,2
è³¾å¯¶çŽ‰,è³¾ç,3
ç§¦å¯å¿,è³¾æ¯,1
çŽ‹å¤«äºº,ç§¦å¯å¿,1
çŽ‹ç†™é³³,è³¾æ¯,4
å¹³å…’,çŽ‹ç†™é³³,2
è³¾æ”¿,è³¾èµ¦,5
è³¾ç,è³¾èµ¦,7
è³¾æ”¿,è³¾ç,5
æž—é»›çŽ‰,è³¾æ¯,1
æž—é»›çŽ‰,è³¾ç’‰,2
ç§¦å¯å¿,è³¾ç’‰,1
çŽ‹ç†™é³³,ç§¦å¯å¿,1
è¥²äºº,è³¾æ¯,3
çŽ‹ç†™é³³,è³¾ç,3
çŽ‹ç†™é³³,è³¾å¯¶çŽ‰,8
æž—é»›çŽ‰,çŽ‹ç†™é³³,1
çŽ‹ç†™é³³,è³¾ç’‰,3
çŽ‹å¤«äºº,è¿Žæ˜¥,2
å¯¶ç ,è³¾ç,1
è³¾å¯¶çŽ‰,è³¾æ¯,5
è–›å¯¶é‡µ,è¿Žæ˜¥,3
çŽ‹å¤«äºº,è–›å¯¶é‡µ,2
çŽ‹ç†™é³³,è–›å¯¶é‡µ,1
çŽ‹ç†™é³³,è¿Žæ˜¥,1
è³¾æ¯,è³¾è“‰,1
è³¾è“‰,è³¾èµ¦,1
è³¾æ¯,è³¾èµ¦,3
æž—é»›çŽ‰,è¿Žæ˜¥,2
æž—é»›çŽ‰,è³¾å¯¶çŽ‰,9
è–›å¯¶é‡µ,è³¾å¯¶çŽ‰,2
æž—é»›çŽ‰,è–›å¯¶é‡µ,4
è³¾å¯¶çŽ‰,è¿Žæ˜¥,2
å¹³å…’,é¦™è±,2
çŽ‹ç†™é³³,é¦™è±,1
è³¾ç,è³¾ç’‰,6
å¹³å…’,è³¾ç’‰,1
çŽ‹ç†™é³³,è³¾è“‰,2
è³¾ç’‰,è³¾èµ¦,3
è³¾æ”¿,è³¾ç’‰,2
è³¾å¯¶çŽ‰,è³¾æ”¿,3
è³¾æ”¿,è³¾æ¯,3
æž—é»›çŽ‰,çŽ‹å¤«äºº,1
è³¾æŽ¢æ˜¥,è¿Žæ˜¥,2
è³¾æ•¬,è³¾èµ¦,1
è³¾æ”¿,è³¾æ•¬,1
è³¾ç,è³³ç’°,1
è³³ç’‰,è³³ç’°,1
è³³ç’°,è³³è“‰,1
è³³ç’‰,è³³è“‰,1
è¥²äºº,è³³å¯¶çŽ‰,4
ç§‹ç´‹,è³³æ¯,1
å²å¤§å§‘å¨˜,è³³æ¯,1
ç§‹ç´‹,é´›é´¦,1
è–›å¯¶é‡µ,é¦™è±,1
è³³æƒœæ˜¥,è¿Žæ˜¥,1
æž—é»›çŽ‰,è³³æŽ¢æ˜¥,1
è³³æƒœæ˜¥,è³³æŽ¢æ˜¥,1
è–›å¯¶é‡µ,è³³æƒœæ˜¥,1
è–›å¯¶é‡µ,è³³æŽ¢æ˜¥,1
æž—é»›çŽ‰,è³³æƒœæ˜¥,1
è³³æ¯,è³³ç’°,1
è–›å¯¶é‡µ,è³³ç’°,1
çŽ‹å¤«äºº,è³³ç’°,1
çŽ‹ç†™é³³,è³³ç’°,1
è±å…’,è¿Žæ˜¥,1
å²å¤§å§‘å¨˜,è–›å¯¶é‡µ,1
ç§¦å¯å¿,è³³è“‰,2
"""

# --- Step 1: Load Data into a DataFrame ---
df = pd.read_csv(io.StringIO(csv_data))

# Display the raw data table
st.dataframe(df)

# --- Step 2: Build the Network ---
st.markdown("### Step 2: Network Visualization")
st.markdown("This interactive network graph shows the relationships between characters. The thickness of edges represents the frequency of interactions.")

# Create a NetworkX graph from the DataFrame
G = nx.Graph()
for index, row in df.iterrows():
    G.add_edge(row['Source'], row['Target'], weight=row['Weight'])

# Visualize the network using PyVis
net = Network(height="600px", width="100%", notebook=True)
net.from_nx(G)
net.show("network.html")

# Display the interactive PyVis network graph
st.write("Interactive Character Network:")
HtmlFile = open("network.html", 'r', encoding='utf-8')
source_code = HtmlFile.read()
st.components.v1.html(source_code, height=600)

# --- Step 3: Display Centrality Metrics ---
st.markdown("### Step 3: Centrality Metrics")
st.markdown("Here we calculate centrality metrics to identify key characters in the network.")

# Degree Centrality
degree_centrality = nx.degree_centrality(G)
st.subheader("Top 5 Characters by Degree Centrality")
top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
st.write(pd.DataFrame(top_degree, columns=["Character", "Degree Centrality"]))

# Betweenness Centrality
betweenness_centrality = nx.betweenness_centrality(G)
st.subheader("Top 5 Characters by Betweenness Centrality")
top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
st.write(pd.DataFrame(top_betweenness, columns=["Character", "Betweenness Centrality"]))

# --- Step 4: Display Gephi Results ---
st.markdown("### Step 4: Gephi Visualization")
st.markdown("""
While the interactive PyVis graph is useful for exploring the network, the **Gephi** results provide a more refined visual representation.

- **Gephi** offers more flexibility in terms of layout, color schemes, and community detection.
- The **modularity** and **color coding** in Gephi highlight the social clusters clearly.
- The thickness and opacity of edges in Gephi make the frequencies of interactions more visually distinct.

Here is the final result generated from **Gephi**:
""")

# Display Gephi-generated image
st.image("Final.png", caption="Gephi Visualization of Character Interactions")

# --- Step 5: Code Walkthrough ---
st.markdown("### Step 5: Code Walkthrough")
st.markdown("In this section, we explain the code used to process the text and generate the network.")
st.markdown("""
1. **Text Processing**: We used **jieba** for tokenizing the text and identifying character names.
2. **Interaction Recording**: We recorded interactions between characters whenever they appeared in the same sentence.
3. **Network Construction**: We built a **NetworkX** graph to represent the relationships, using **PyVis** to create an interactive visualization.
4. **Centrality Calculations**: We used **NetworkX** to calculate metrics like **degree centrality** and **betweenness centrality** to identify key characters.
""")

# Display the full code for processing the text and generating the CSV
code = '''
import jieba
import re
from collections import Counter
import csv

base_directory = r'C:\\Users\\puipu\\Documents\\MScGAH\\CHC5904\\Second Hands on Assignment'

with open(f'{base_directory}\\chapters_10_20.txt', 'r', encoding='utf-8') as file:
    text = file.read()

character_variations = {
    'è³ˆå¯¶çŽ‰': ['è³ˆå¯¶çŽ‰', 'å¯¶çŽ‰', 'çŽ‰å…„', 'è³ˆå…¬å­', 'è³ˆäºŒçˆº'],
    'æž—é»›çŽ‰': ['æž—é»›çŽ‰', 'é»›çŽ‰', 'æž—å¦¹å¦¹', 'æž—å§‘å¨˜', 'é»›'],
    'è–›å¯¶é‡µ': ['è–›å¯¶é‡µ', 'å¯¶é‡µ', 'è–›å°å§', 'è–›å§‘å¨˜', 'é‡µå§', 'è–›å§¨åª½'],
    'è¥²äºº': ['è¥²äºº', 'èŠ±è¥²äºº', 'è¥²å§‘å¨˜'],
    'é´›é´¦': ['é´›é´¦', 'é´›é´¦å§'],
    'è³ˆæ¯': ['è³ˆæ¯', 'è€ç¥–å®—', 'è³ˆè€å¤ªå›', 'è³ˆè€ç¥–', 'è€å¤ªå¤ª', 'å¤ªå›'],
    'è³ˆæ”¿': ['è³ˆæ”¿', 'æ”¿è€çˆº', 'è³ˆäºŒè€çˆº'],
    'çŽ‹ç†™é³³': ['çŽ‹ç†™é³³', 'é³³å§', 'é³³ä¸«é ­', 'ç†™é³³'],
    'è³ˆç’‰': ['è³ˆç’‰', 'ç’‰äºŒçˆº', 'ç’‰å“¥', 'è³¾ç’‰äºŒçˆº'],
    # Add more characters and their variations here...
}

def find_characters_in_window(window):
    present_characters = set()
    for main_character, variations in character_variations.items():
        if any(variation in window for variation in variations):
            present_characters.add(main_character)
    return present_characters

sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)

interactions = []

for sentence in sentences:
    tokens = jieba.lcut(sentence)  
    present_characters = find_characters_in_window(tokens) 
    
    if len(present_characters) > 1: 
        sorted_pairs = set()
        for character1 in present_characters:
            for character2 in present_characters:
                if character1 != character2: 
                    sorted_pair = tuple(sorted([character1, character2]))
                    sorted_pairs.add(sorted_pair)
        interactions.extend(sorted_pairs)

interaction_counts = Counter(interactions)

with open(f'{base_directory}\\character_interactions_with_nicknames.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(['Source', 'Target', 'Weight'])
    for (source, target), weight in interaction_counts.items():
        csvwriter.writerow([source, target, weight])

print("Character interactions saved to 'character_interactions_with_nicknames.csv'.")
'''

st.code(code, language="python")

st.markdown("This is the full Python code used for processing the text and generating the graph.")
